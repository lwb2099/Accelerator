{
    // 使用 IntelliSense 了解相关属性。 
    // 悬停以查看现有属性的描述。
    // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "1.train_summac",
            "type": "python",
            "module": "accelerate.commands.launch",
            "request": "launch",
            // "program": "1. multi-GPU-training-with-accelerator/train_summac.py",
            "console": "integratedTerminal",
            "justMyCode": true,
            "args": [
                "--main_process_port", "25910",
                "--config_file", "1. multi-GPU-training-with-accelerator/summac_config.yaml",
                "1. multi-GPU-training-with-accelerator/summac/summac/train_summac.py",
                "--model", "vitc",
                "--granularity", "sentence", 
                "--train_batch_size", "8", 
                "--num_epochs", "10",
                "--nli_labels", "e"
            ]
        },
        {
            "name": "2.train_bart_trainer",
            "type": "python",
            "request": "launch",
            "program": "2. pre-train-with-bart-text-infilling/train_bart_pipeline.py",
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "2.train_bart_no_trainer",
            "type": "python",
            "request": "launch",
            "module": "accelerate.commands.launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--main_process_port", "25910",
                "--config_file",  "2. pre-train-with-bart-text-infilling/no_deepspeed.yaml",  // "2. pre-train-with-bart-text-infilling/deepspeed_config.yaml",
                "2. pre-train-with-bart-text-infilling/train_bart.py",
                "--dataset_name", "bookcorpus",
                "--cache_dir", "2. pre-train-with-bart-text-infilling/datasets",
                "--model_name_or_path","facebook/bart-base",
                "--output_dir", "2. pre-train-with-bart-text-infilling/models/ckpt",
                "--max_train_data", "100000",
                "--max_eval_data", "1000",
                "--with_tracking",
                "--report_to", "wandb",
                "--log_interval", "100",
                "--checkpointing_steps", "1000",
                "--per_device_train_batch_size", "4",
                "--per_device_eval_batch_size", "2",
                "--model_dir", "2. pre-train-with-bart-text-infilling/models",
                "--max_seq_length", "256",
                "--mlm_probability", "0.0",
            ]
        },
        {
            "name": "3.relative_position_example",
            "type": "python",
            "request": "launch",
            "program": "3. Self-Attention with Relative Position Representations/relative_position_example.py",
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "3.example_fairseq_train",
            "type": "python",
            "request": "launch",
            "program": "3. Self-Attention with Relative Position Representations/fairseq/fairseq_cli/train.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "./3. Self-Attention with Relative Position Representations/fairseq/examples/translation/data-bin/iwslt14.tokenized.de-en",
                "--arch",
                "transformer_iwslt_de_en",
                "--share-decoder-input-output-embed",
                "--optimizer",
                "adam",
                "--adam-betas",
                "(0.9, 0.98)",
                "--clip-norm",
                "0.0",
                "--lr",
                "5e-4",
                "--lr-scheduler",
                "inverse_sqrt",
                "--warmup-updates",
                "4000",
                "--dropout",
                "0.3",
                "--weight-decay",
                "0.0001",
                "--criterion",
                "label_smoothed_cross_entropy",
                "--label-smoothing",
                "0.1",
                "--max-tokens",
                "4096",
                "--eval-bleu",
                "--eval-bleu-args",
                "{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}",
                "--eval-bleu-detok",
                "moses",
                "--eval-bleu-remove-bpe",
                "--eval-bleu-print-samples",
                "--best-checkpoint-metric",
                "bleu",
                "--maximize-best-checkpoint-metric",
                "--distributed-world-size",
                "1",
            ]
        },
        {
            "name": "3.relative_attn_fairseq_train",
            "type": "python",
            "request": "launch",
            "program": "3. Self-Attention with Relative Position Representations/fairseq/fairseq_cli/train.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "3. Self-Attention with Relative Position Representations/fairseq/examples/translation/data-bin/iwslt14.tokenized.de-en",
                "--arch",
                "transformer_iwslt_de_en",
                "--share-decoder-input-output-embed",
                "--use-relative-attn",
                "--no-token-positional-embeddings",
                "--optimizer",
                "adam",
                "--adam-betas",
                "(0.9, 0.98)",
                "--clip-norm",
                "0.0",
                "--lr",
                "5e-4",
                "--lr-scheduler",
                "inverse_sqrt",
                "--warmup-updates",
                "4000",
                "--dropout",
                "0.3",
                "--weight-decay",
                "0.0001",
                "--criterion",
                "label_smoothed_cross_entropy",
                "--label-smoothing",
                "0.1",
                "--max-tokens",
                "4096",
                "--eval-bleu",
                "--eval-bleu-args",
                "{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}",
                "--eval-bleu-detok",
                "moses",
                "--eval-bleu-remove-bpe",
                "--eval-bleu-print-samples",
                "--best-checkpoint-metric",
                "bleu",
                "--maximize-best-checkpoint-metric",
                "--distributed-world-size",
                "1",
            ]
        }
    ]
}